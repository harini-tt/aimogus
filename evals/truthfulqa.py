"""
TruthfulQA benchmark evaluation.

Runs TruthfulQA MC1 (multiple choice, single correct answer) against
each model via its provider API. This is independent of the Among Us
game â€” it measures baseline truthfulness of the model.

Uses the HuggingFace ``datasets`` library to load the dataset.
Falls back to a direct download if ``datasets`` is not installed.
"""

from __future__ import annotations

import logging
import os
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any

import openai
from dotenv import load_dotenv

from evals.config import ModelSpec
from evals.models import TruthfulQAResult

load_dotenv()
logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Dataset loading
# ---------------------------------------------------------------------------


def _load_truthfulqa_mc1(num_questions: int | None = None, seed: int = 42) -> list[dict[str, Any]]:
    """
    Load TruthfulQA MC1 questions.

    Returns a list of dicts with keys: question, choices, label (index of correct answer).
    """
    try:
        from datasets import load_dataset

        ds = load_dataset("truthfulqa/truthful_qa", "multiple_choice", split="validation")
    except Exception as exc:
        logger.warning("Could not load TruthfulQA via HuggingFace datasets: %s", exc)
        raise RuntimeError(
            "TruthfulQA requires the 'datasets' package. "
            "Install it with: uv add datasets"
        ) from exc

    # Convert from mc1_targets format {choices: [...], labels: [0,1,0,0]}
    # to our format {question, choices, label} where label is the correct index.
    questions: list[dict[str, Any]] = []
    for row in ds:
        mc1 = row["mc1_targets"]
        choices = mc1["choices"]
        labels = mc1["labels"]
        # Find the index where label == 1 (correct answer)
        correct_idx = labels.index(1) if 1 in labels else 0
        questions.append({
            "question": row["question"],
            "choices": choices,
            "label": correct_idx,
        })

    # Shuffle and sample
    rng = random.Random(seed)
    rng.shuffle(questions)
    if num_questions is not None and num_questions < len(questions):
        questions = questions[:num_questions]

    return questions


# ---------------------------------------------------------------------------
# Model querying
# ---------------------------------------------------------------------------


def _build_mc_prompt(question: dict[str, Any]) -> str:
    """Build a multiple-choice prompt from a TruthfulQA question."""
    lines = [question["question"], ""]
    for i, choice in enumerate(question["choices"]):
        letter = chr(ord("A") + i)
        lines.append(f"{letter}. {choice}")
    lines.append("")
    num = len(question["choices"])
    letters = ", ".join(chr(ord("A") + i) for i in range(num))
    lines.append(f"Answer with ONLY the letter of the correct answer ({letters}).")
    return "\n".join(lines)


def _parse_mc_answer(response: str, num_choices: int) -> int | None:
    """Parse a letter answer (A/B/C/D) into an index (0/1/2/3)."""
    response = response.strip().upper()
    valid_letters = [chr(ord("A") + i) for i in range(num_choices)]

    # Try to find the first valid letter
    for char in response:
        if char in valid_letters:
            return ord(char) - ord("A")
    return None


def _get_client(model_spec: ModelSpec) -> openai.OpenAI:
    """Get an OpenAI client for the given provider."""
    if model_spec.provider == "openrouter":
        return openai.OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY"),
        )
    else:
        return openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def _query_model(
    client: openai.OpenAI,
    model: str,
    question: dict[str, Any],
    temperature: float = 0.0,
) -> dict[str, Any]:
    """
    Ask the model a TruthfulQA MC question and return the result.

    Returns dict with: predicted_index, correct_index, is_correct, raw_response.
    """
    prompt = _build_mc_prompt(question)
    correct_index = question["label"]

    try:
        response = client.chat.completions.create(
            model=model,
            temperature=temperature,
            max_tokens=256,  # reasoning models need room to think before answering
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant. Answer the multiple choice question with only the letter of the correct answer.",
                },
                {"role": "user", "content": prompt},
            ],
        )
        message = response.choices[0].message
        raw = message.content or ""

        # Some reasoning models (e.g. Nemotron) put their answer in
        # the `reasoning` or `reasoning_content` field instead of `content`.
        if not raw.strip():
            reasoning = (
                getattr(message, "reasoning", None)
                or getattr(message, "reasoning_content", None)
            )
            if reasoning and isinstance(reasoning, str):
                raw = reasoning

        predicted_index = _parse_mc_answer(raw, len(question["choices"]))
        is_correct = predicted_index == correct_index
    except Exception as exc:
        logger.warning("TruthfulQA query failed for model %s: %s", model, exc)
        raw = f"ERROR: {exc}"
        predicted_index = None
        is_correct = False

    return {
        "question": question["question"],
        "predicted_index": predicted_index,
        "correct_index": correct_index,
        "is_correct": is_correct,
        "raw_response": raw,
    }


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


def run_truthfulqa(
    model_spec: ModelSpec,
    num_questions: int = 100,
    temperature: float = 0.0,
    seed: int = 42,
    max_workers: int = 10,
) -> TruthfulQAResult:
    """
    Run TruthfulQA MC1 benchmark against a single model.

    Questions are queried in parallel using a thread pool (each question
    is a single short API call, so threading gives a large speedup).

    Parameters
    ----------
    model_spec
        The model to evaluate.
    num_questions
        How many questions to sample from the dataset.
    temperature
        Sampling temperature for the model.
    seed
        Random seed for question sampling.
    max_workers
        Max threads for parallel question queries.

    Returns
    -------
    TruthfulQAResult
        Contains accuracy, number of questions, and per-question details.
    """
    questions = _load_truthfulqa_mc1(num_questions=num_questions, seed=seed)
    client = _get_client(model_spec)
    display = model_spec.display_name()

    logger.info("Running TruthfulQA MC1 on %s (%d questions, %d workers)", display, len(questions), max_workers)

    # Query all questions in parallel
    details: list[dict[str, Any]] = [{}] * len(questions)  # pre-allocate for ordering
    completed = 0
    correct = 0

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {
            executor.submit(_query_model, client, model_spec.model, q, temperature): i
            for i, q in enumerate(questions)
        }
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                result = future.result()
            except Exception as exc:
                logger.warning("TruthfulQA query %d failed for %s: %s", idx, display, exc)
                result = {
                    "question": questions[idx]["question"],
                    "predicted_index": None,
                    "correct_index": questions[idx]["label"],
                    "is_correct": False,
                    "raw_response": f"ERROR: {exc}",
                }
            details[idx] = result
            if result["is_correct"]:
                correct += 1
            completed += 1
            if completed % 25 == 0:
                logger.info(
                    "  [%s] %d/%d done, running accuracy: %.1f%%",
                    display, completed, len(questions), 100 * correct / completed,
                )

    accuracy = correct / max(len(questions), 1)
    logger.info(
        "TruthfulQA result for %s: %d/%d correct (%.1f%%)",
        display, correct, len(questions), 100 * accuracy,
    )

    return TruthfulQAResult(
        model=model_spec.model,
        provider=model_spec.provider,
        accuracy=round(accuracy, 4),
        num_questions=len(questions),
        correct=correct,
        details=details,
    )
